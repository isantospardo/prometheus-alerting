# Condition to avoid sending alerts to the openshift-admins for the non production clusters
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-main
type: Opaque
stringData:
  alertmanager.yaml: |-
    # Configuration file for alertmanager
    global:
      resolve_timeout: {{ .Values.alertManager.resolveTimeout }}

    # The root route on which each incoming alert enters.
    # The root route with all parameters, which are inherited by the child
    # routes if they are not overwritten.
    route:
      group_by: ['cluster', 'alertname']
{{- if .Values.alertManager.enableEmailReceiver }}
      receiver: 'email'
{{- else }}
      receiver: 'null'
{{- end }}
      group_wait: {{ .Values.alertManager.groupWait }}
      group_interval: {{ .Values.alertManager.groupInterval }}
      repeat_interval: {{ .Values.alertManager.repeatInterval }}
      routes:
        # The Watchdog alert is intended to fire at all times. It makes sure that the entire alerting pipeline actually works.
        # Basically it verifies, that Prometheus can reach Alertmanager, and Alertmanager can reach the notification provider.
        # For this, we redirect this alarm to a black hole email address, in order to not receive dozens of notifications.
        - match_re:
            alertname: Watchdog
          receiver: 'email_nowhere'
    receivers:
    - name: 'null'
    - name: 'email_nowhere'
      email_configs:
      - to: {{ .Values.alertManager.emailConfig.emailNowhere }}
        from: {{ .Values.alertManager.emailConfig.emailFrom }}
        smarthost: {{ .Values.alertManager.emailConfig.smartHost }}
    - name: 'email'
      email_configs:
      - to: {{ .Values.alertManager.emailConfig.emailTo }}
        from: {{ .Values.alertManager.emailConfig.emailFrom }}
        smarthost: {{ .Values.alertManager.emailConfig.smartHost }}
        headers:
          From: {{ .Values.alertManager.emailConfig.emailFrom }}
          Subject: '[OKD4][{{ .Values.clusterName }}]{{`{{ template "email.default.subject" . }}`}}'
